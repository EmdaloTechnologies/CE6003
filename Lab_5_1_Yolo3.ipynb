{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab_5_1_Yolo3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3rj6mWPfkNV",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# Object Detection with Yolo v3\n",
        "\n",
        "The [*You Only Look Once* detector (YOLO)](https://pjreddie.com/darknet/yolo/) is a real-time object detection system based on CNNs.  It is extremely fast - over 100x faster than Fast-RCNN.\n",
        "\n",
        "It applies a single neural network to the full image, dividing the image into regions and jointly predicts bounding boxes and probabilities for each region. These bounding boxes are weighted by the predicted probabilities.\n",
        "\n",
        "![YOLO v3](https://pjreddie.com/media/image/yologo_2.png)\n",
        "\n",
        "## qqwweee implementation of YOLOv3\n",
        "This lab is based on the following implementation of Yolo in Keras - https://github.com/qqwweee/keras-yolo3.\n",
        "\n",
        "We'll clone the Mask R-CNN code directly from the GitHub repository.\n",
        "If we have run this step already, you'll see an error of the form fatal: destination path 'Mask_RCNN' already exists and is not an empty directory which you can safely ignore."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymWTbTWUfq4G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/qqwweee/keras-yolo3.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1evhE7aa4x7f",
        "colab_type": "text"
      },
      "source": [
        "First, we'll load TensorFlow v1.x, as required by this YOLOv3 model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Lub8cyT4yR5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEDr9IfXxsYY",
        "colab_type": "text"
      },
      "source": [
        "*To* ensure these labs run as fast as possible, from the menu above select **Edit > Notebook settings or Runtime > Change runtime type** and select GPU as the Hardware accelerator option.\n",
        "\n",
        "Let's test that we are running using the GPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pjk-6QUKxtoD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.test.gpu_device_name()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zUnwF97xvri",
        "colab_type": "text"
      },
      "source": [
        "And now we can load Keras and the rest of the Python libraries we need into our notebook runtime."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-FDxeq1gOvd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "\n",
        "from keras import backend as K\n",
        "from keras.models import load_model\n",
        "from keras.layers import Input\n",
        "\n",
        "ROOT_DIR = os.path.abspath(\"./keras-yolo3\")\n",
        "sys.path.append(ROOT_DIR)\n",
        "\n",
        "from yolo3.utils import *\n",
        "from yolo3.model import *\n",
        "\n",
        "# And various libraries for image manipulation and plotting\n",
        "import random\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import urllib\n",
        "import matplotlib.patches as patches\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrL7D8ppjztF",
        "colab_type": "text"
      },
      "source": [
        "## Converting pre-trained weights\n",
        "\n",
        "We can leverage some weights that are pre-trained from the the YOLO website - https://pjreddie.com/darknet/yolo/. YOLO is originally developed in an open source neural network framework called Darknet, so we needt o convert the weights from the Darknet format into the HDF5 format that TensorFlow/Keras can read.\n",
        "\n",
        "Our pre-trained model is trained on the Microsoft COCO dataset. ![COCO Examples](http://cocodataset.org/images/coco-examples.jpg)\n",
        "\n",
        "COCO has several features:\n",
        "\n",
        "* Object segmentation;\n",
        "* Recognition in context;\n",
        "* Superpixel stuff segmentation;\n",
        "* 330K images (>200K labeled);\n",
        "* 1.5 million object instances;\n",
        "* 80 object categories;\n",
        "* 91 stuff categories;\n",
        "* 5 captions per imag\n",
        "* 250,000 people with keypoints.\n",
        "\n",
        "You can read more about COCO at http://cocodataset.org/#home\n",
        "\n",
        "HDF5 (.h5, .hdf5) is a file format suitable for storing large multidimensional numeric arrays (e.g. models, data files). HDF stands for Hierarchical Data Format, and can store everything about your model, including:\n",
        "\n",
        "* The architecture of the model;\n",
        "* The weights of the model;\n",
        "* The training configuration (loss, optimizer);\n",
        "* The state of the optimizer, so you can resume training exactly where you left off.\n",
        "\n",
        "You can read more about the HDF5 file format at http://docs.h5py.org/en/stable/quick.html, and at the Keras API for loading and saving models at https://keras.io/getting-started/faq/#how-can-i-save-a-keras-model\n",
        "\n",
        "\n",
        "The weights are about 240MB to download. The conversion will take a minute.\n",
        "\n",
        "**Note:** As the model we are leveraging uses Tensorflow 1.x, you can safely ignore some warnings about deprecated functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNHqOKZ7fd39",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://pjreddie.com/media/files/yolov3.weights -O keras-yolo3/yolov3.weights\n",
        "!python keras-yolo3/convert.py keras-yolo3/yolov3.cfg keras-yolo3/yolov3.weights keras-yolo3/model_data/yolo.h5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfOXjxSqCi6F",
        "colab_type": "text"
      },
      "source": [
        "Next, we need to setup a class_names array for our class detections, and a set of anchors.\n",
        "\n",
        "The YOLO model returns a class index to represent the class, but we need the array to convert from the index number into a human readable string. These are just the standard 80 classes from COCO.\n",
        "\n",
        "YOLO also uses anchor boxes - based on the intuition that most bounding boxes have common width to height ratios. Instead of predicting bounding boxes directly, YOLO works of a predetermined set of box sizes, called anchor boxes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WS-eV4KUz4Cv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We need to load our COCO class names\n",
        "class_names = ['person', 'bicycle', 'car', 'motorcycle', 'airplane',\n",
        "               'bus', 'train', 'truck', 'boat', 'traffic light',\n",
        "               'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird',\n",
        "               'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear',\n",
        "               'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie',\n",
        "               'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n",
        "               'kite', 'baseball bat', 'baseball glove', 'skateboard',\n",
        "               'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup',\n",
        "               'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',\n",
        "               'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n",
        "               'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed',\n",
        "               'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote',\n",
        "               'keyboard', 'cell phone', 'microwave', 'oven', 'toaster',\n",
        "               'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors',\n",
        "               'teddy bear', 'hair drier', 'toothbrush']\n",
        "\n",
        "# YOLO uses anchor boxes - setup our array of common width/heights\n",
        "anchors = np.array([[ 10,  13],\n",
        "       [ 16,  30],\n",
        "       [ 33,  23],\n",
        "       [ 30,  61],\n",
        "       [ 62,  45],\n",
        "       [ 59, 119],\n",
        "       [116,  90],\n",
        "       [156, 198],\n",
        "       [373, 326]])\n",
        "anchors = np.divide(anchors, 1.0) # convert our int array to float"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGjtyHTYC9DZ",
        "colab_type": "text"
      },
      "source": [
        "Next, we'll build our YOLO model.  This uses the TensorFlow 1.x *session*-style API semantics.  You don't need to worry too much about this, as the TF2.x Keras style API is much nicer.\n",
        "\n",
        "**Note:** As the model we are leveraging uses Tensorflow 1.x, you can safely ignore some warnings about deprecated functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvHZoudnfgCb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_classes = len(class_names)\n",
        "\n",
        "# Set the expected image size for the model - the model expects an image size \n",
        "# where both the width and height are multiples of 32\n",
        "model_image_size = (416, 416)\n",
        "assert model_image_size[0]%32 == 0, 'Multiples of 32 required'\n",
        "assert model_image_size[1]%32 == 0, 'Multiples of 32 required'\n",
        "\n",
        "# Create YOLO model\n",
        "model_path = os.path.join(ROOT_DIR, \"model_data/yolo.h5\")\n",
        "yolo_model = load_model(model_path, compile=False)\n",
        "\n",
        "# Disable warnings about deprecated TF 1.x functions vis-a-vis TF 2.x API\n",
        "try:\n",
        "    from tensorflow.python.util import module_wrapper as deprecation\n",
        "except ImportError:\n",
        "    from tensorflow.python.util import deprecation_wrapper as deprecation\n",
        "deprecation._PER_MODULE_WARNING_LIMIT = 0\n",
        "\n",
        "# Generate output tensor targets for bounding box predictions\n",
        "# \n",
        "# Predictions for individual objects are based on a probability score \n",
        "# threshold of 0.3, and an IoU threshold for non-max suppression of 0.45\n",
        "#\n",
        "# When run, this will evaluate the YOLO model on given inputs, and return\n",
        "# filtered bounding boxes to us\n",
        "\n",
        "input_image_shape = K.placeholder(shape=(2, ))\n",
        "boxes, scores, classes = yolo_eval(yolo_model.output, anchors, num_classes, \n",
        "                                   input_image_shape, score_threshold=0.3,\n",
        "                                   iou_threshold=0.45)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbauX60grl7s",
        "colab_type": "text"
      },
      "source": [
        "We'll create a helper function to detect objects in our image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bUF1HnFh30u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def detect_objects_in_image(image):\n",
        "    # we convert out image data to 32-bit float, normalise it (between 0.0 and 0.1)\n",
        "    image_data = np.array(image, dtype='float32')\n",
        "    image_data /= 255.\n",
        "    image_data = np.expand_dims(image_data, 0)  # Add batch dimension.\n",
        "\n",
        "    # Predict classes and locations using Tensorflow session\n",
        "    sess = K.get_session()\n",
        "    out_boxes, out_scores, out_classes = sess.run(\n",
        "                [boxes, scores, classes],\n",
        "                feed_dict={\n",
        "                    yolo_model.input: image_data,\n",
        "                    input_image_shape: [image.size[1], image.size[0]],\n",
        "                    K.learning_phase(): 0\n",
        "                })\n",
        "    return out_boxes, out_scores, out_classes\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPuEeeztsCss",
        "colab_type": "text"
      },
      "source": [
        "And finally, a helper function to draw bounding boxes and labels on our original images and display them, so that we can see the results of our inference."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gx_uAnacrkql",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def draw_detected_objects_in_image(image, out_boxes, out_scores, out_classes):\n",
        "    # Set up some display formatting\n",
        "    cmap = plt.get_cmap('tab20b') # select a dark qualitative colormap for labels/bounding boxes\n",
        "    colors = [cmap(i) for i in np.linspace(0, 1, 20)]\n",
        "\n",
        "    # Plot the image\n",
        "    img = np.array(image)\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots(1, figsize=(12,9))\n",
        "    ax.imshow(img)\n",
        "\n",
        "    # Use a random color for each class\n",
        "    unique_labels = np.unique(out_classes)\n",
        "    n_cls_preds = len(unique_labels)\n",
        "    bbox_colors = random.sample(colors, n_cls_preds)\n",
        "\n",
        "    # process each instance of each class that was found\n",
        "    for instance, class_index in reversed(list(enumerate(out_classes))):\n",
        "        predicted_class = class_names[class_index]\n",
        "        bbox = out_boxes[instance]\n",
        "        score = out_scores[instance]\n",
        "\n",
        "\n",
        "        # Unpack the bounding box coordinates\n",
        "        (y1, x1, y2, x2) = bbox\n",
        "\n",
        "        # Set the box dimensions\n",
        "        box_h = (y2 - y1) \n",
        "        box_w = (x2 - x1)\n",
        "        \n",
        "        # Add a box with the color for this class\n",
        "        color = bbox_colors[int(np.where(unique_labels == class_index)[0])]\n",
        "        bbox = patches.Rectangle((x1, y1), box_w, box_h, linewidth=4, edgecolor=color, facecolor='none')\n",
        "        ax.add_patch(bbox)\n",
        "        \n",
        "        # Format the label to be added to the image for this instance and add it\n",
        "        label = 'Class: {}, Score: {:.2f}'.format(predicted_class, score)\n",
        "        plt.text(x1, y1, s=label, color='white', verticalalignment='top',\n",
        "                bbox={'color': color, 'pad': 0})\n",
        "        \n",
        "    plt.axis('off')\n",
        "    plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Xth3HuYvyqx",
        "colab_type": "text"
      },
      "source": [
        "Okay, we now have enough boilerplate code written to grab a number of test images, and run them through our model to see how we get on.\n",
        "\n",
        "For the sake of simplicity, we're cheating hugely here as we're just using sample images that are taken from the COCO dataset (so our model has already been exposed to them during training, and would be expected to do really well on them), but feel free to pick other image URLs and add them to the array below to run them through YOLO.\n",
        "\n",
        "What is so impressive about deep convolutional classifiers and detectors is that, once they are fed with sufficient training data, they are unfazed by occlusions (where part of the object is hidden or obscured from view) or pose."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sm2x_YziQGY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def url_to_image(url):\n",
        "\tresp = urllib.request.urlopen(url)\n",
        "\ttemp_image = np.asarray(bytearray(resp.read()), dtype=\"uint8\")\n",
        "\ttemp_image = cv2.imdecode(temp_image, cv2.IMREAD_COLOR)\n",
        "\ttemp_image = cv2.cvtColor(temp_image, cv2.COLOR_BGR2RGB) # OpenCV defaults to BGR, but we need RGB here..\n",
        "\tpil_image = Image.fromarray(temp_image)\n",
        "\treturn pil_image\n",
        "\n",
        "image_urls = [\"https://farm9.staticflickr.com/8319/7885077674_6901d14828_z.jpg\", \n",
        "              \"https://farm8.staticflickr.com/7201/6879489797_fa772d8a69_z.jpg\", \n",
        "              \"https://farm4.staticflickr.com/3830/9381322299_698270f4d3_z.jpg\",\n",
        "              \"https://farm8.staticflickr.com/7331/9280720567_b684d5cccf_z.jpg\",\n",
        "              \"http://farm4.staticflickr.com/3175/2961808668_1d557e34a0_z.jpg\",\n",
        "              \"http://farm3.staticflickr.com/2441/3541164845_6fbea2e89f_z.jpg\",\n",
        "              \"http://farm8.staticflickr.com/7296/9019745657_c8776db96f_z.jpg\",\n",
        "              \"http://farm8.staticflickr.com/7108/6901315528_676d32186e_z.jpg\",\n",
        "              \"http://farm9.staticflickr.com/8101/8514154431_b8f1b57dc9_z.jpg\"\n",
        "              ]\n",
        "\n",
        "for image_url in image_urls:\n",
        "  print(\"Considering image\", image_url)\n",
        "  image = url_to_image(image_url)\n",
        "  \n",
        "  if (image.size != model_image_size):\n",
        "    print(\"\\tResizing (with letterbox) from \", image.size, \" to \", model_image_size)\n",
        "    image = letterbox_image(image, tuple(reversed(model_image_size)))\n",
        "\n",
        "  out_boxes, out_scores, out_classes = detect_objects_in_image(image)\n",
        "  draw_detected_objects_in_image(image, out_boxes, out_scores, out_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46oNwO5ZvcQ6",
        "colab_type": "text"
      },
      "source": [
        "**Exercise:** Look for more image URLs via the COCO image explorer http://cocodataset.org/#explore or use Google.  Then paste the URLs into the test array `image_urls` in the cell above, and re-run the detections."
      ]
    }
  ]
}