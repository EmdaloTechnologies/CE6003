{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Lab_7_1_RecursiveBayesianFilter.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baqq6FcdVNTD",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# Recursive Bayes Filter - Lab 7.1\n",
        "\n",
        "## Recap\n",
        "\n",
        "This is the Lab on using a Recursive Bayes Filter in CE6003's Object Tracking. You should complete the tasks in this lab as part of the Bayes section of the lesson.\n",
        "\n",
        "Please remember this lab must be completed before taking the quiz at the end of this lesson.\n",
        "\n",
        "First, if we haven't already done so, we need to clone the various images and resources needed to run these labs into our workspace.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWrzzv1DVwbu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/EmdaloTechnologies/CE6003.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXyxCpyTVzlu",
        "colab_type": "text"
      },
      "source": [
        "This program demonstrates a very simple 'tracking' mechanism - derived from a Bayesian approach.\n",
        "\n",
        "It shows the improvements to an estimate for the position of a static object (such as a car) as new position measurements (e.g. from a GPS system) arrive.\n",
        "\n",
        "It illustrates the shape of the $(x,y)$ measurements arriving to the algorithm as a histogram of $x$, a histogram of $y$ - each with mean $\\mu$ and variance $\\sigma^2$, a scatter plot of the measurements arriving and as a covariance $\\Sigma$ of $\\theta$ a vector containing $x$ and $y$.  These are terms and concepts we'll use throughout the lessons."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9--h9Em3xMUc",
        "colab_type": "text"
      },
      "source": [
        "First, lets import our typical libaries; numpy, scipy, math for matrix maths, matplotlib, mplot3d for plotting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5UKegQGXK-Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Our imports\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "import matplotlib.cm as cm\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import scipy.stats as stats\n",
        "import os\n",
        "import math\n",
        "from IPython import display\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6_-9iCHXXcB",
        "colab_type": "text"
      },
      "source": [
        "**Program Structure**\n",
        "\n",
        "After visualising the input data, the program simply\n",
        "loops *iterations* times; each time refining its *position* estimate based on an array of pre-generated *measurements*, fed to it one set at a time.\n",
        "\n",
        "the actual position is 4,5 but the program doesn't know that\n",
        "- instead it build a region where it becomes increasing confident that, for each update, the car should be (from a series of estimates centred around 4,5 where the estimation error is assumed to be normal or gaussian).\n",
        "\n",
        "In this example, this assumption is valid as the estimation error is **defined** as a gaussian distribution (in both $x$ and $y$) around where the car actually is."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGqewbSnyV_4",
        "colab_type": "text"
      },
      "source": [
        "**Major Model Variables**\n",
        "\n",
        "The major variables that control the model are:\n",
        "\n",
        "**iterations:** how may times to run the model\n",
        "\n",
        "**gridsize:** the size of the x-y grid to place the car in\n",
        "\n",
        "**actual_pos:** where the car actually is on the grid\n",
        "\n",
        "**variance:** the variance in the measurements (both in x and y)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9u7no5GyXyFj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# major model variables\n",
        "iterations = 50         # number of iterations to run the model\n",
        "gridsize=(40,40)        # the size of the grid containing our car\n",
        "actual_pos = (4,5)      # the actual position of the car in the grid\n",
        "\n",
        "meas_variance = (2,2)   # the variances (in x and y) of the measurement estimates"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0hZJPc5X4Fb",
        "colab_type": "text"
      },
      "source": [
        "**Estimates**\n",
        "\n",
        "Create the estimates up front - ensure they are centred around actual_pos and the estimates vary in a gaussianÂ manner with the variances defined in <code>meas_variance</code> above"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8CmN_6oX8Gd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "estimates = np.zeros((iterations,2), dtype=float)\n",
        "estimates[:,0] = np.random.normal(actual_pos[0], meas_variance[0], iterations)\n",
        "estimates[:,1] = np.random.normal(actual_pos[1], meas_variance[1], iterations)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTWEPuAtYGvZ",
        "colab_type": "text"
      },
      "source": [
        "# Visualising the input data\n",
        "\n",
        "This shows visualisations of the data. It illustrates the estimates as a histogram in $x$ with mean $\\mu_x$ and variance $\\sigma^2_x$, a histogram in $y$ with mean $\\mu_y$ and variance $\\sigma^2_y$ and a scatter-plot showing the estimates centred around the actual value of $x$ and $y$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7leWxDXYUYJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create 2 x 2 sub plots\n",
        "\n",
        "gs = gridspec.GridSpec(2,2)\n",
        "plt.figure(figsize=np.array([210,297]) / 25.4)\n",
        "ax = plt.subplot(gs[1, 0]) # row 1, col 0\n",
        "ax.set_title('X Position Estimates Histogram')\n",
        "ax.hist(estimates[:,0],density=True,bins=30)\n",
        "ax.set(xlabel='Weight', ylabel = 'Probability')\n",
        "\n",
        "ay = plt.subplot(gs[1,1]) # row 1, col 1\n",
        "ay.set_title('Y Position Estimates Histogram')\n",
        "ay.hist(estimates[:,1],density=True,bins=30)\n",
        "ay.set(xlabel='Weight', ylabel = 'Probability')\n",
        "\n",
        "sc = plt.subplot(gs[0, :]) # row 0, span all columns\n",
        "sc.scatter(actual_pos[0], actual_pos[1],color='red', s=150)\n",
        "sc.scatter(estimates[:,0],estimates[:,1],color='blue')\n",
        "sc.set_title('Scatter-Plot of Position Estimates')\n",
        "sc.set(xlabel = 'X', ylabel='Y')\n",
        "plt.show()\n",
        "plt.close('all')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0aLfvSyE5W2",
        "colab_type": "text"
      },
      "source": [
        "**Position Estimates**\n",
        "\n",
        "Here, we're forming an intuition about how the estimates will arrive, over time, to our Recursive Bayesian Filter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6A6cbexlYloe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Intuition as to how data will arrive into the algorithm\n",
        "min_x = min(estimates[:,0])\n",
        "max_x = max(estimates[:,0])\n",
        "min_y = min(estimates[:,1])\n",
        "max_y = max(estimates[:,1])\n",
        "fig = plt.figure()\n",
        "\n",
        "plt.grid(True)\n",
        "plt.scatter(actual_pos[0], actual_pos[1],color='red', s=150)\n",
        "scat = plt.scatter(estimates[:0,0],estimates[:0,1],color='blue')\n",
        "plt.title('Scatter-Plot of Position Estimates')\n",
        "plt.xlabel = 'X'\n",
        "plt.ylabel='Y'\n",
        "plt.xlim(min_x-2, max_x+2)\n",
        "plt.ylim(min_y-2, max_y+2)\n",
        "\n",
        "ax = fig.gca()\n",
        "\n",
        "display.clear_output(wait=True)\n",
        "\n",
        "for i in range(iterations):\n",
        "        plt.grid(True)\n",
        "        plt.scatter(actual_pos[0], actual_pos[1],color='red', s=150)\n",
        "        plt.scatter(estimates[:i,0],estimates[:i,1],color='blue')\n",
        "        plt.title('Scatter-Plot of Position Estimates')\n",
        "        plt.xlabel = 'X'\n",
        "        plt.ylabel='Y'\n",
        "        plt.xlim(min_x-2, max_x+2)\n",
        "        plt.ylim(min_y-2, max_y+2)\n",
        "        display.display(plt.gcf())\n",
        "        display.clear_output(wait=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2kCl-DoFWcl",
        "colab_type": "text"
      },
      "source": [
        "**Covariance**\n",
        "\n",
        "In this plot, we're going to visualise the covariance in the x and y terms of our estimates.\n",
        "\n",
        "One key takeaway from this - before we run the estimates through our RBF is to note the spread of the covariance.  We'll be hoping to show that the RBF reduces this covariance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-IK6O6eYyq-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Visualize the probabilities of the estimates\n",
        "# as a 3D height map\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.gca(projection='3d')\n",
        "hist, xedges, yedges = np.histogram2d(estimates[:,0], \\\n",
        "                                      estimates[:,1], bins=(30,30))\n",
        "X, Y = np.meshgrid(xedges[:-1], yedges[:-1])\n",
        "pos = np.empty(X.shape + (2,))\n",
        "pos[:, :, 0] = X; pos[:, :, 1] = Y\n",
        "rv = stats.multivariate_normal(actual_pos, \\\n",
        "                               [[meas_variance[0], 0], [0, meas_variance[1]]])\n",
        "surf = ax.plot_surface(X,Y, rv.pdf(pos), \\\n",
        "                       cmap=cm.coolwarm, linewidth=0, antialiased=False)\n",
        "ax.set_xlabel('X')\n",
        "ax.set_ylabel('Y')\n",
        "ax.set_zlabel('Probability')\n",
        "\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dxgv6jDSY2Vw",
        "colab_type": "text"
      },
      "source": [
        "**Recursive Bayesian Filter (RBF)**\n",
        "\n",
        "Now, we are going to iteratively solve:\n",
        "\n",
        "$$\\displaystyle P(A\\mid B)={\\frac {P(B \\mid A) \\times P(A)}{P(B)}} $$\n",
        "\n",
        "where $A$ and $B$ are events (and $P(B)$ is non-zero)\n",
        "\n",
        "<br>\n",
        "\n",
        "$P(A\\mid B)$ is a *conditional probability*:\n",
        "\n",
        "       what is the likelihood of seeing A given B is true\n",
        "\n",
        "<br>\n",
        "\n",
        "$P(B\\mid  A)$ is also a *conditional probability*:\n",
        "\n",
        "       what is the likelihood of B given A is true\n",
        "\n",
        "<br>\n",
        "\n",
        "$P(A)$ and $P(B)$ are the probabilities of seeing $A$ and $B$ independently of each other.\n",
        "\n",
        "<br>\n",
        "\n",
        "Or, put another way:\n",
        "\n",
        "For each point in the grid, on the arrival of a new estimate, the probability that it is the **right** point is affected by its old probability and how likely it is that it can account for the new estimate.\n",
        "\n",
        "<br>\n",
        "\n",
        "  <center>$\\displaystyle{post = \\frac{(prior \\times likelihood)}{normalisation}}$</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PjeuVssGScR",
        "colab_type": "text"
      },
      "source": [
        "**Initialisation of Terms**\n",
        "\n",
        "We're defining a prior term where we create a grid (in the shape of gridsize, one point for each position in the grid) and we initialise that to $\\frac{1}{num-gridpoints}$, i.e. all squares in the grid are equally likely to be the correct square.\n",
        "\n",
        "<br>\n",
        "\n",
        "We create a post with the same shape as the prior and initialise it to the prior.\n",
        "\n",
        "<br>\n",
        "\n",
        "Now we want to handle the uncertainty in our state.  Effectively we're going to treat this as deciding how confident we are in our position relative to our measurement.\n",
        "\n",
        "<br>\n",
        "\n",
        "We create a covariance term as a 2 by 2 matrix, looking like this:\n",
        "\n",
        "<br>\n",
        "\n",
        "<center>$K = \\begin{bmatrix}0.1 & 0 \\\\ 0 & 0.1\\end{bmatrix}$</center>\n",
        "\n",
        "<br>\n",
        "\n",
        "Where we are assuming our variances are completely independent. \n",
        "\n",
        "<br>\n",
        "\n",
        "You might recall that the full covariance term is typically described as:\n",
        "\n",
        "<br>\n",
        "\n",
        "<center>$K = \\begin{bmatrix}\\sigma^2_x & \\sigma_x\\sigma_y \\\\ \\sigma_y\\sigma_x & \\sigma^2_y\\end{bmatrix}$</center>\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "So, in our model, the $K_{1,1}$ term ($\\sigma^2_x$) will only affect the x-term of our position estimate and the $K_{2,2}$ term ($\\sigma^2_y$) only affects the y-term of our position estimate and we have zeros for our $K_{1,2}$ and $K_{2,1}$ terms, stating there is no dependence between the behaviour of our $x$ and $y$ terms.\n",
        "\n",
        "So, to summarise, $K_{1,1}$ and $K_{2,2}$ are set to a variance of 0.1 in our model and the $K_{1,2}$ and $K_{2,1}$ terms are set to zero."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMeOEIuHbimw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define a prior matrix with one point for every point\n",
        "# on the grid.\n",
        "prior = np.zeros(shape=gridsize)\n",
        "# nothing is known at this stage so all squares are equally likely\n",
        "# initialise all with same probability (1 / num squares)\n",
        "prior = prior + 1/(gridsize[0] * gridsize[1])\n",
        "\n",
        "# Create a post matrix\n",
        "post = np.zeros(shape=gridsize)\n",
        "# set to same value as priors for now\n",
        "post = prior\n",
        "\n",
        "# define a covariance matrix K for making a 2-D Gaussian variance with\n",
        "# shape as described above\n",
        "K = np.eye(2) * 0.1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LubDhlssImdT",
        "colab_type": "text"
      },
      "source": [
        "**Exercise 1**\n",
        "\n",
        "Adjust the covariance in the K term above and monitor its affect in the animation below. For example, multiply by 10 and divide by 10.\n",
        "\n",
        "**Exercise 2**\n",
        "Adjust the measurement covariance term $(2,2)$ above and monitor its affect on the animation below.\n",
        "\n",
        "**Insight**\n",
        "These two exercises illustrate the key relationship in the Recursive Bayesian Filter (RBF) - the impact of the covariance of position estimate vs measurement covariance on the behaviour of the model.\n",
        "\n",
        "**Exercise 3**\n",
        "Generate the position estimates differently.  Instead of 50 estimates around a single mean, create 100 estimates around one mean (e.g. 5,4) and append another 100 estimates around a second mean (e.g. 4,5).  Observe how the filter copes with this.\n",
        "\n",
        "**Insight**\n",
        "The insight here is that we can see a route where we can use the RBF (or a variation thereof) to take account of a moving object."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "velFQ0nlKKqt",
        "colab_type": "text"
      },
      "source": [
        "**Grid Adjustment**\n",
        "\n",
        "To make the grid more generic, we leave the grid based on the size of grid in gridsize, but we set the range and interval of the grid to something sensible, for instance we could say that our grid consists of 40 values in x and 40 values in y; and that the values start at half the value of the x-value we are estimating and the x values end at 1.5 times the x-value we are estimating and that the interval is linear between these two values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDkT5tAhKGMQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define a grid (defined as gridsize), initialise all points\n",
        "x_range = np.zeros(gridsize[0])\n",
        "y_range = np.zeros(gridsize[1])\n",
        "\n",
        "# initialise x_range with values from x0 .. X .. xn such that \n",
        "# actual_pos is in the middle of the grid and the grid is\n",
        "# scaled in a reasonable manner\n",
        "min_x = actual_pos[0] - (actual_pos[0])/2\n",
        "max_x = actual_pos[0] + (actual_pos[0])/2\n",
        "step_x = (max_x - min_x) / gridsize[0]\n",
        "for i in range(gridsize[0]):\n",
        "        x_range[i] = min_x + i*step_x\n",
        "\n",
        "# initialise y_range with values from y0 .. Y .. yn such \n",
        "# that actual_pos is in the middle and the grid is scaled in\n",
        "# a reasonable manner\n",
        "min_y = actual_pos[1] - (actual_pos[1])/2\n",
        "max_y = actual_pos[1] + (actual_pos[1])/2\n",
        "step_y = (max_y - min_y) / gridsize[1]\n",
        "for i in range(gridsize[1]):\n",
        "        y_range[i] = min_y + i*step_y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejKZBFjUIgv_",
        "colab_type": "text"
      },
      "source": [
        "**Main Loop of Recursive Bayesian Filter**\n",
        "\n",
        "On each iteration, this visits each square on the grid and determines how likely that this square is to be the 'correct' square, given its history and the new estimate.\n",
        "\n",
        "We generate a likelihood term for how likely it is to see the new measurement from that square.\n",
        "\n",
        "We multiply that likelihood term by the square's prior - effectively a measure of how 'confident' the square was that it was the 'correct' square.\n",
        "\n",
        "We crudely normalise the $(prior \\times likelihood)$ term, by normalising to 1. We simply sum all $(prior \\times likelihood)$ terms in the grid and divide each of those $(prior \\times likelihood)$ terms by that sum.\n",
        "\n",
        "Re-summing will now equal to 1 - so we've crudely converted back into a probability distribution - effectively now we can recurse infinitely.\n",
        "\n",
        "<br>\n",
        "\n",
        "***Likelihood Calculation***\n",
        "\n",
        "If you are interested in the maths behind the likelihood calculation, I've assembled some notes here; for those who are not interested, you can simply retain that we're using a simple bivariate normal probability function and skip over this section.\n",
        "\n",
        "We compute the likelihood of receiving a measurement in a particular grid square by assuming that our $x$ and $y$ terms are each normally distributed and independent, therefore we can assume a joint normal distribution and use a multivariate probability density function (PDF) as follows:\n",
        "\n",
        "<br>\n",
        "\n",
        "<center>${P(\\theta|\\mu,\\Sigma) = \\frac{1}{\\sqrt{(2\\pi)^k|\\Sigma|}} e^{-\\frac{1}{2}(\\theta-\\mu)^T\\Sigma^{-1}(\\theta-\\mu)}}$&emsp;&emsp;&emsp;(Equation 1)</center>\n",
        "\n",
        "<br>\n",
        "\n",
        "* See en.wikipedia.org/wiki/Multivariate_normal_distribution, Density Function etc for derivation.\n",
        "\n",
        "<br>\n",
        "\n",
        "where:\n",
        "\n",
        "&emsp;$k$ is the number of jointly normally distributed variables\n",
        "\n",
        "&emsp;$\\theta$ is a column vector of $k$ possible jointly distributed variables\n",
        "\n",
        "&emsp;$\\Sigma$ is the covariance matrix for those variables\n",
        "\n",
        "&emsp;$\\mu$ is $E[\\theta]$, i.e. a vector of arithmetic means for each variable\n",
        "\n",
        "&emsp;$|\\Sigma|$ is the determinant of the covariance matrix $\\Sigma$\n",
        "\n",
        "<br>\n",
        "\n",
        "***Bivariate Density Function***\n",
        "\n",
        "Now, we'll simplify the general multivariate probability density function to our specialist case.\n",
        "\n",
        "In the bivariate case (where $k = 2$) and for two variables, named $(x, y)$, we can define $\\mu$ as follows:\n",
        "\n",
        "<br>\n",
        "\n",
        "<center>$\\mu = \\begin{pmatrix}\\mu_{x} \\\\ \\mu_{y}\\end{pmatrix}$</center>\n",
        "\n",
        "<br>\n",
        "\n",
        "and, again for two variables, we can define covariance $\\Sigma$ as follows:\n",
        "\n",
        "<br>\n",
        "\n",
        "<center>$\\Sigma = \\begin{pmatrix}\\sigma^2_{x} && \\sigma_{x}\\sigma_{y} \\\\\n",
        "  \\sigma_{y}\\sigma_{x} && \\sigma_{y}^2 \\end{pmatrix}$</center>\n",
        "\n",
        "<br>\n",
        "\n",
        "We're assuming that $x$ and $y$ our two variables are mutually uncorrelated, thus they are completely independent of each other; in this case we can replace the $\\sigma_x\\sigma_y$ and $\\sigma_y\\sigma_x$ terms with 0 as shown here:\n",
        "\n",
        "<br>\n",
        "\n",
        "<center>$\\Sigma = \\begin{pmatrix}\\sigma^2_{x} && 0 \\\\\n",
        "  0 && \\sigma_{y}^2 \\end{pmatrix}$</center>\n",
        "\n",
        "<br>\n",
        "\n",
        "And this has the property that the determinant of this covariance matrix $(|\\Sigma|)$ is $\\sigma^2_x\\sigma^2_y$ which if you undertake to derive from first principles is convenient.\n",
        "\n",
        "<br>\n",
        "\n",
        "Starting with Equation 1 above, we now know $k$, the number of variables, so we can supply 2 in place of $k$:\n",
        "\n",
        "<br>\n",
        "\n",
        "<center>${P(\\theta|\\mu,\\Sigma) = \\frac{1}{\\sqrt{(2\\pi)^2|\\Sigma|}} e^{-\\frac{1}{2}(\\theta-\\mu)^T\\Sigma^{-1}(\\theta-\\mu)}}$</center>\n",
        "\n",
        "<br>\n",
        "\n",
        "and, thus we can move the $2\\pi$ term outside the square root, as shown here.\n",
        "\n",
        "<br>\n",
        "\n",
        "<center>${P(\\theta|\\mu,\\Sigma) = \\frac{1}{2\\pi\\sqrt{|\\Sigma|}} e^{-\\frac{1}{2}(\\theta-\\mu)^T\\Sigma^{-1}(\\theta-\\mu)}}$</center>\n",
        "\n",
        "<br>\n",
        "\n",
        "In the code below, we will calculate the two pieces of the function as follows:\n",
        "\n",
        "<center>$constant = \\frac{1}{2\\pi\\sqrt(|\\Sigma|}$</center>\n",
        "\n",
        "<center>$exp = e^{-\\frac{1}{2}(\\theta - \\mu)^T\\Sigma^1(\\theta - \\mu)}$</center>\n",
        "and then combine.\n",
        "\n",
        "We will use <code>np</code>'s routines to calculate the determinate, the transpose, and the inverse and we will reduce a $2x1$ by $2x2$ by $2x2$ set of matrix operations to a single value.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbV-dAkSbrhK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "display.clear_output(wait=True)\n",
        "\n",
        "#\n",
        "# On each iteration:\n",
        "#       For each point:\n",
        "#               How likely is it that this point can 'explain' the new estimate\n",
        "#               Multiply that by how confident the point was\n",
        "#\n",
        "#       Convert all new values to a prob dist by ensuring they total to 1.\n",
        "#\n",
        "for iteration in range(iterations):\n",
        "        prior = post    # store the (old) post to the prior\n",
        "        m = 0 * prior   # m is our working area and starts at zero\n",
        "\n",
        "        # likelihood algorithm\n",
        "        #       look at each location.\n",
        "        #       assume that location is the correct location\n",
        "        #       get the likelihood of the point accounting for the\n",
        "        #       estimate  assuming 2-D gaussian noise\n",
        "        for i in range(gridsize[0]):\n",
        "                for j in range(gridsize[1]):\n",
        "                        # compute likelihood\n",
        "\n",
        "                        # this represents where we 'think' we are\n",
        "                        pos = np.array([x_range[i], y_range[j]])\n",
        "                        constant = 1 / ((2 * np.pi) * np.sqrt (np.linalg.det(K)))\n",
        "\n",
        "                        estimate = estimates[iteration]\n",
        "                        est_term = np.matmul((estimate-pos), np.linalg.inv(K))\n",
        "                        est_term = np.matmul(est_term, (estimate-pos).transpose())\n",
        "                        exp = np.exp(-1/2 * est_term)\n",
        "                        likelihood = constant * exp\n",
        "\n",
        "                        # how likely we are to see this estimate\n",
        "                        m[i,j] = likelihood\n",
        "\n",
        "                        ## combine this likelihood with prior confidence\n",
        "                        m[i,j] = m[i,j] * prior[i,j]\n",
        "\n",
        "        # normalise this distribution to make it\n",
        "        # a probability distribution\n",
        "        post = m / np.sum(m)\n",
        "        \n",
        "        # Pretty pictures - plot Post\n",
        "        fig = plt.figure()\n",
        "        ax = fig.gca(projection='3d')\n",
        "        x = x_range\n",
        "        y = y_range\n",
        "\n",
        "        X, Y = np.meshgrid(x, y)\n",
        "        surf = ax.plot_surface(X,Y, post, cmap=cm.coolwarm, linewidth=0, antialiased=False)\n",
        "        ax.set_xlabel('X')\n",
        "        ax.set_ylabel('Y')\n",
        "        ax.set_zlabel('Probability')\n",
        "        ax.set_zlim(0,0.8)\n",
        "\n",
        "        display.display(plt.gcf())\n",
        "        display.clear_output(wait=True)\n",
        "        plt.close()\n",
        "\n",
        "plt.close('all')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNph9HbP-Tnh",
        "colab_type": "text"
      },
      "source": [
        "*Conclusion*\n",
        "\n",
        "The key takeaways are:\n",
        "\n",
        "1. We have introduced the concept that we are treating our position and our estimates as probability terms - defined by their mean and variance (standard deviation squared).\n",
        "2. We have the developed the concept of using Bayes Theorem to successively improve an estimate\n",
        "3. We have applied that insight to locating a static object, iteratively using uncertain initial position and uncertain measurements.\n",
        "4. We have gained the insight that one or two 'poor' measurements do not significantly adjust our position confidence.\n",
        "5. We have introduced three key terms - $posterior$ or $post$, $prior$ and $measurement$, where each term is essentially conceived as a mean and variance about that mean of that term.  We're treating $prior$ as essentially confidence in position before $measurement$ arrival, $measurement$ as essentially a confidence in a measurement, and $post$ as essentially a new confidence in position after $measurement$.\n",
        "6. We've demonstrated how to express that for a vector length of 2 (bivariate) which we will build on in later lessons.\n",
        "\n",
        "*Next Steps*\n",
        "1. Kalman Filters\n",
        "2. Particle Filters"
      ]
    }
  ]
}